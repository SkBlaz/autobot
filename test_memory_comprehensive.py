#!/usr/bin/env python3
"""
Comprehensive test to demonstrate memory optimizations
Tests multiple sizes and measures memory efficiency
"""

import autoBOTLib
import pandas as pd
import psutil
import os
import gc
import time

def get_memory_usage():
    """Get current memory usage in MB"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024

def test_progressive_sizes():
    """Test progressively larger dataset sizes to demonstrate memory handling"""
    
    # Load the full dataset
    try:
        dataframe = pd.read_csv("data/insults/train.tsv", sep="\t")
        full_sequences = dataframe['text_a']
        full_targets = dataframe['label']
        
        print(f"Full dataset: {len(full_sequences)} samples")
        
    except Exception as e:
        print(f"Could not load full dataset: {e}")
        return False
    
    # Test different sizes
    sizes_to_test = [100, 250, 500, 750, 1000, 1500]
    
    results = []
    
    for size in sizes_to_test:
        if size > len(full_sequences):
            print(f"Skipping size {size} (exceeds dataset size)")
            continue
            
        print(f"\n=== Testing with {size} samples ===")
        
        # Get subset
        train_sequences = full_sequences.head(size)
        train_targets = full_targets.head(size)
        
        # Initial memory
        gc.collect()
        initial_memory = get_memory_usage()
        print(f"Initial memory: {initial_memory:.1f} MB")
        
        start_time = time.time()
        
        try:
            # Initialize with optimized settings
            autoBOTLibObj = autoBOTLib.GAlearner(
                train_sequences,
                train_targets,
                representation_type="symbolic",  # Memory efficient
                n_fold_cv=3,
                sparsity=0.4,  # Higher sparsity for memory efficiency  
                time_constraint=0.01,  # Very short
                hof_size=1,  # Small hall of fame
                num_cpu=2,  # Limit CPU usage
                verbose=0,  # Reduce logging
                memory_storage="memory"
            )
            
            after_init_memory = get_memory_usage()
            memory_increase = after_init_memory - initial_memory
            
            # Train
            autoBOTLibObj.evolve(strategy="direct-learning")
            
            after_train_memory = get_memory_usage()
            
            # Test prediction
            test_data = train_sequences.head(min(10, size))
            predictions = autoBOTLibObj.predict(test_data)
            
            end_time = time.time()
            final_memory = get_memory_usage()
            
            # Record results
            result = {
                'size': size,
                'initial_memory_mb': initial_memory,
                'peak_memory_mb': final_memory,
                'memory_increase_mb': final_memory - initial_memory,
                'memory_per_sample_kb': (final_memory - initial_memory) * 1024 / size,
                'training_time_s': end_time - start_time,
                'predictions': len(predictions),
                'status': 'SUCCESS'
            }
            
            print(f"‚úì Peak memory: {final_memory:.1f} MB (+{final_memory - initial_memory:.1f} MB)")
            print(f"‚úì Memory per sample: {result['memory_per_sample_kb']:.1f} KB/sample")
            print(f"‚úì Training time: {result['training_time_s']:.1f}s")
            print(f"‚úì Predictions: {len(predictions)}")
            
            # Cleanup
            del autoBOTLibObj
            del train_sequences, train_targets, predictions
            gc.collect()
            
        except Exception as e:
            result = {
                'size': size,
                'initial_memory_mb': initial_memory,
                'peak_memory_mb': get_memory_usage(),
                'memory_increase_mb': get_memory_usage() - initial_memory,
                'memory_per_sample_kb': 0,
                'training_time_s': time.time() - start_time,
                'predictions': 0,
                'status': f'FAILED: {str(e)[:100]}'
            }
            print(f"‚úó Failed: {e}")
        
        results.append(result)
        
        # Force cleanup between tests
        gc.collect()
        time.sleep(1)
    
    # Print summary
    print("\n" + "="*80)
    print("MEMORY OPTIMIZATION TEST SUMMARY")
    print("="*80)
    print(f"{'Size':<6} {'Memory (MB)':<12} {'KB/Sample':<12} {'Time (s)':<10} {'Status':<15}")
    print("-" * 80)
    
    successful_tests = 0
    for result in results:
        status_short = result['status'][:12] if len(result['status']) <= 12 else result['status'][:12]
        print(f"{result['size']:<6} {result['peak_memory_mb']:<12.1f} {result['memory_per_sample_kb']:<12.1f} {result['training_time_s']:<10.1f} {status_short:<15}")
        if result['status'] == 'SUCCESS':
            successful_tests += 1
    
    print(f"\nSuccessful tests: {successful_tests}/{len(results)}")
    
    if successful_tests > 0:
        # Calculate memory efficiency
        successful_results = [r for r in results if r['status'] == 'SUCCESS']
        if len(successful_results) > 1:
            largest_success = max(successful_results, key=lambda x: x['size'])
            print(f"Largest successful dataset: {largest_success['size']} samples")
            print(f"Memory efficiency: {largest_success['memory_per_sample_kb']:.1f} KB per sample")
            
        return True
    else:
        print("No successful tests - memory optimizations may need further work")
        return False

if __name__ == "__main__":
    print("Running comprehensive memory optimization test...")
    success = test_progressive_sizes()
    
    if success:
        print("\nüéâ Memory optimization improvements are working!")
        print("   - The system can now handle larger datasets")
        print("   - Memory usage is more predictable and controlled")
        print("   - Proper cleanup prevents memory leaks")
    else:
        print("\n‚ùå Memory optimization test failed")
        print("   - Further improvements may be needed")